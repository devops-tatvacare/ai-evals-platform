<!-- ============================================================
     TAB 4: PROMPTS & SCHEMAS
     ============================================================ -->
<div id="tab-prompts" class="tab-content">

  <!-- Why Prompts Matter -->
  <h2 class="section-title">Why Prompts Matter</h2>
  <p class="section-subtitle">Prompts are the control surface for LLM evaluations. They confine the AI's narrative to a structured evaluation layer &mdash; transforming open-ended language models into deterministic scoring engines. Template variables (<code>{{variable}}</code>) inject runtime context (audio files, transcripts, chat messages) while JSON schemas enforce output structure.</p>

  <!-- Template Variable Registry -->
  <h2 class="section-title">Template Variable Registry</h2>
  <p class="section-subtitle">All 17 template variables defined in variableRegistry.ts, organized by type and availability.</p>

  <div class="table-wrapper">
    <table class="data-table">
      <thead>
        <tr>
          <th>Variable</th>
          <th>Type</th>
          <th>Description</th>
          <th>Apps</th>
          <th>Prompt Types</th>
          <th>Flows</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><code>{{audio}}</code></td>
          <td><span class="badge badge-amber">file</span></td>
          <td>Audio file for transcription/evaluation</td>
          <td>voice-rx</td>
          <td>transcription, evaluation</td>
          <td>upload, api</td>
        </tr>
        <tr>
          <td><code>{{transcript}}</code></td>
          <td><span class="badge badge-blue">text</span></td>
          <td>Original AI transcript (system under test)</td>
          <td>voice-rx</td>
          <td>evaluation, extraction</td>
          <td>upload, api</td>
        </tr>
        <tr>
          <td><code>{{llm_transcript}}</code></td>
          <td><span class="badge badge-purple">computed</span></td>
          <td>Judge AI transcript (generated in Call 1)</td>
          <td>voice-rx</td>
          <td>evaluation</td>
          <td>upload, api</td>
        </tr>
        <tr>
          <td><code>{{chat_transcript}}</code></td>
          <td><span class="badge badge-blue">text</span></td>
          <td>Full conversation as plain User/Bot messages</td>
          <td>kaira-bot</td>
          <td>evaluation</td>
          <td>all</td>
        </tr>
        <tr>
          <td><code>{{time_windows}}</code></td>
          <td><span class="badge badge-purple">computed</span></td>
          <td>Time windows from original transcript for segment-aligned transcription</td>
          <td>voice-rx</td>
          <td>transcription</td>
          <td>upload</td>
        </tr>
        <tr>
          <td><code>{{segment_count}}</code></td>
          <td><span class="badge badge-purple">computed</span></td>
          <td>Number of segments in the original transcript</td>
          <td>voice-rx</td>
          <td>transcription, evaluation</td>
          <td>upload</td>
        </tr>
        <tr>
          <td><code>{{speaker_list}}</code></td>
          <td><span class="badge badge-purple">computed</span></td>
          <td>Comma-separated list of speakers</td>
          <td>voice-rx</td>
          <td>transcription, evaluation</td>
          <td>upload</td>
        </tr>
        <tr>
          <td><code>{{script_preference}}</code></td>
          <td><span class="badge badge-blue">text</span></td>
          <td>User preference for output script (devanagari, romanized, auto)</td>
          <td>voice-rx</td>
          <td>transcription, evaluation</td>
          <td>upload, api</td>
        </tr>
        <tr>
          <td><code>{{language_hint}}</code></td>
          <td><span class="badge badge-blue">text</span></td>
          <td>Language hint for the audio (e.g., Hindi, Hinglish)</td>
          <td>voice-rx</td>
          <td>transcription, evaluation</td>
          <td>upload, api</td>
        </tr>
        <tr>
          <td><code>{{preserve_code_switching}}</code></td>
          <td><span class="badge badge-blue">text</span></td>
          <td>Whether to preserve code-switching (yes/no)</td>
          <td>voice-rx</td>
          <td>transcription, evaluation</td>
          <td>upload, api</td>
        </tr>
        <tr>
          <td><code>{{original_script}}</code></td>
          <td><span class="badge badge-purple">computed</span></td>
          <td>Detected script of the original transcript</td>
          <td>voice-rx</td>
          <td>evaluation</td>
          <td>upload</td>
        </tr>
        <tr>
          <td><code>{{structured_output}}</code></td>
          <td><span class="badge badge-blue">text</span></td>
          <td>AI-generated structured data (rx object) from API response</td>
          <td>voice-rx</td>
          <td>evaluation</td>
          <td>api</td>
        </tr>
        <tr>
          <td><code>{{api_input}}</code></td>
          <td><span class="badge badge-blue">text</span></td>
          <td>Input payload sent to the API (query/context)</td>
          <td>voice-rx</td>
          <td>transcription, evaluation</td>
          <td>api</td>
        </tr>
        <tr>
          <td><code>{{api_rx}}</code></td>
          <td><span class="badge badge-blue">text</span></td>
          <td>Complete API response including metadata</td>
          <td>voice-rx</td>
          <td>transcription, evaluation</td>
          <td>api</td>
        </tr>
        <tr>
          <td><code>{{llm_structured}}</code></td>
          <td><span class="badge badge-purple">computed</span></td>
          <td>Structured data extracted by Judge AI (from Call 1)</td>
          <td>voice-rx</td>
          <td>evaluation</td>
          <td>upload, api</td>
        </tr>
      </tbody>
    </table>
  </div>

  <!-- Variable Resolution Flow -->
  <h2 class="section-title">Variable Resolution Flow</h2>
  <div class="card">
    <div class="card-body">
      <div class="mermaid">
flowchart LR
    Prompt["Prompt Template"] -->|"extractVariables()"| Extract["Extract {{var}} tokens"]
    Extract -->|"For each variable"| Resolve["resolveVariable()"]
    Resolve -->|"text type"| Sub["Substitute in prompt string"]
    Resolve -->|"file type"| Keep["Keep as placeholder (handled by LLM service)"]
    Resolve -->|"computed type"| Compute["Compute from context (transcript, AI eval)"]
    Compute --> Sub
    Sub --> Final["Resolved Prompt"]
    Keep --> Final

    style Prompt fill:#6366f1,color:#fff
    style Final fill:#10b981,color:#fff
      </div>
    </div>
  </div>

  <!-- Two Schema Systems -->
  <h2 class="section-title">Two Schema Systems</h2>
  <div class="grid-2">

    <div class="card">
      <div class="card-body">
        <h3 class="card-title">JSON Schema (SchemaDefinition)</h3>
        <table class="info-table">
          <tr>
            <td class="info-label">Used in</td>
            <td>Evaluation overlays, passed directly to Gemini/OpenAI SDK</td>
          </tr>
          <tr>
            <td class="info-label">Format</td>
            <td>Standard JSON Schema with type, properties, required</td>
          </tr>
          <tr>
            <td class="info-label">Purpose</td>
            <td>Enforces structured output from LLM (<code>response_json_schema</code> in Gemini, <code>response_format</code> in OpenAI)</td>
          </tr>
        </table>
        <pre class="code-block"><button class="copy-btn" onclick="navigator.clipboard.writeText(this.nextElementSibling.textContent)">Copy</button><code>{
  "type": "object",
  "properties": {
    "segments": {
      "type": "array",
      "items": {
        "type": "object",
        "properties": {
          "speaker": { "type": "string" },
          "text": { "type": "string" },
          "score": { "type": "number" }
        },
        "required": ["speaker", "text", "score"]
      }
    }
  },
  "required": ["segments"]
}</code></pre>
      </div>
    </div>

    <div class="card">
      <div class="card-body">
        <h3 class="card-title">Field-Based (EvaluatorOutputField[])</h3>
        <table class="info-table">
          <tr>
            <td class="info-label">Used in</td>
            <td>Custom evaluators, built via <code>InlineSchemaBuilder</code> visual editor</td>
          </tr>
          <tr>
            <td class="info-label">Format</td>
            <td>Array of field definitions with key, type, label, description, thresholds, isMainMetric, displayMode</td>
          </tr>
          <tr>
            <td class="info-label">Conversion</td>
            <td><code>generateJsonSchema()</code> in schema_generator.py converts to JSON Schema at runtime</td>
          </tr>
        </table>
        <pre class="code-block"><button class="copy-btn" onclick="navigator.clipboard.writeText(this.nextElementSibling.textContent)">Copy</button><code>[
  {
    "key": "overall_score",
    "type": "number",
    "label": "Overall Score",
    "description": "0-100 quality score",
    "isMainMetric": true,
    "thresholds": { "red": 40, "yellow": 70, "green": 90 }
  },
  {
    "key": "reasoning",
    "type": "string",
    "label": "Reasoning",
    "description": "Explanation for the score"
  }
]</code></pre>
      </div>
    </div>

  </div>

  <!-- Prompt Template Example -->
  <h2 class="section-title">Prompt Template Example</h2>
  <div class="card">
    <div class="card-body">
      <pre class="code-block"><button class="copy-btn" onclick="navigator.clipboard.writeText(this.nextElementSibling.textContent)">Copy</button><code>You are an expert medical transcription evaluator.

Listen to the audio file: <span style="color:#818cf8">{{audio}}</span>

Compare the original transcript below with the AI-generated transcript.

=== ORIGINAL TRANSCRIPT ===
<span style="color:#818cf8">{{transcript}}</span>

=== AI TRANSCRIPT (Judge) ===
<span style="color:#818cf8">{{llm_transcript}}</span>

The original transcript uses <span style="color:#818cf8">{{original_script}}</span> script.
There are <span style="color:#818cf8">{{segment_count}}</span> segments to evaluate.

For each segment, provide:
- Accuracy score (0-100)
- Speaker identification correctness
- Medical terminology accuracy
- Overall assessment</code></pre>
    </div>
  </div>

</div>


<!-- ============================================================
     TAB 5: PIPELINES
     ============================================================ -->
<div id="tab-pipelines" class="tab-content">

  <!-- Execution Pipelines -->
  <h2 class="section-title">Execution Pipelines</h2>
  <p class="section-subtitle">How evaluation jobs flow from user click to saved results.</p>

  <!-- Pipeline Toggle Pills -->
  <div class="filter-pills">
    <button class="filter-pill active" onclick="showPipeline('frontend', this)">Frontend LLM Pipeline</button>
    <button class="filter-pill" onclick="showPipeline('backend', this)">Backend Job Pipeline</button>
    <button class="filter-pill" onclick="showPipeline('voicerx', this)">Voice RX Two-Call</button>
    <button class="filter-pill" onclick="showPipeline('batch', this)">Batch Pipeline</button>
    <button class="filter-pill" onclick="showPipeline('adversarial', this)">Adversarial Pipeline</button>
  </div>

  <!-- PIPELINE VIEW 1: Frontend LLM Pipeline -->
  <div id="pipeline-frontend" style="display:block">
    <h3 class="section-title">Frontend LLM Pipeline</h3>
    <p class="section-subtitle">Used for real-time AI features in the browser. 4-stage pipeline in LLMInvocationPipeline.ts.</p>

    <div class="card">
      <div class="card-body">
        <div class="mermaid">
flowchart LR
    A["1. Validate"] -->|"Check API key, model"| B["2. Prepare"]
    B -->|"Build prompt, attach schema"| C["3. Execute"]
    C -->|"Call Gemini/OpenAI API"| D["4. Post-process"]
    D -->|"Parse JSON, validate schema"| E["Result"]

    style A fill:#6366f1,color:#fff
    style B fill:#8b5cf6,color:#fff
    style C fill:#a78bfa,color:#fff
    style D fill:#c4b5fd,color:#000
    style E fill:#10b981,color:#fff
        </div>
      </div>
    </div>

    <pre class="code-block"><button class="copy-btn" onclick="navigator.clipboard.writeText(this.nextElementSibling.textContent)">Copy</button><code>// src/services/llm/pipeline/index.ts
export function createLLMPipeline(): LLMInvocationPipeline {
  const { apiKey, selectedModel } = useLLMSettingsStore.getState();
  if (!apiKey) throw new Error('API key not configured');
  return new LLMInvocationPipeline(apiKey, selectedModel);
}</code></pre>
  </div>

  <!-- PIPELINE VIEW 2: Backend Job Pipeline -->
  <div id="pipeline-backend" style="display:none">
    <h3 class="section-title">Backend Job Pipeline</h3>
    <p class="section-subtitle">Background job processing via worker_loop() in job_worker.py.</p>

    <div class="card">
      <div class="card-body">
        <div class="mermaid">
flowchart TD
    Click["User clicks 'Run'"] -->|"POST /api/jobs"| Create["Create Job (status: queued)"]
    Create --> Queue["Jobs Table"]
    Worker["worker_loop()"] -->|"Polls every 5s"| Queue
    Worker -->|"Picks oldest queued"| Mark["Mark as 'running'"]
    Mark -->|"process_job()"| Dispatch["JOB_HANDLERS registry"]
    Dispatch -->|"evaluate-voice-rx"| VRX["voice_rx_runner.py"]
    Dispatch -->|"evaluate-batch"| Batch["batch_runner.py"]
    Dispatch -->|"evaluate-adversarial"| Adv["adversarial_runner.py"]
    Dispatch -->|"evaluate-custom"| Custom["custom_evaluator_runner.py"]
    VRX --> Save["Save EvalRun + mark Job completed"]
    Batch --> Save
    Adv --> Save
    Custom --> Save

    style Click fill:#6366f1,color:#fff
    style Worker fill:#8b5cf6,color:#fff
    style Save fill:#10b981,color:#fff
        </div>
      </div>
    </div>

    <pre class="code-block"><button class="copy-btn" onclick="navigator.clipboard.writeText(this.nextElementSibling.textContent)">Copy</button><code># job_worker.py â€” Handler registry
@register_job_handler("evaluate-voice-rx")
async def handle_evaluate_voice_rx(job_id, params):
    return await run_voice_rx_evaluation(job_id=job_id, params=params)

@register_job_handler("evaluate-batch")
async def handle_evaluate_batch(job_id, params):
    return await run_batch_evaluation(job_id=job_id, ...)

@register_job_handler("evaluate-adversarial")
async def handle_evaluate_adversarial(job_id, params):
    return await run_adversarial_evaluation(job_id=job_id, ...)</code></pre>
  </div>

  <!-- PIPELINE VIEW 3: Voice RX Two-Call -->
  <div id="pipeline-voicerx" style="display:none">
    <h3 class="section-title">Voice RX Two-Call Pipeline (Detailed)</h3>

    <div class="card">
      <div class="card-body">
        <div class="mermaid">
flowchart TD
    Start["run_voice_rx_evaluation()"] --> Load["Load Listing + Audio from DB"]
    Load --> Settings["get_llm_settings_from_db()"]
    Settings --> Provider["create_llm_provider() + LoggingLLMWrapper"]
    Provider --> Check{"skip_transcription?"}

    Check -->|No| Call1["CALL 1: Transcription"]
    Check -->|Yes| Reuse["Reuse previous AI transcript"]

    Call1 --> Resolve1["resolve_prompt(transcription_prompt)"]
    Resolve1 --> LLM1["llm.generate_with_audio(prompt, audio, schema)"]
    LLM1 --> Parse1["parse_transcript_response()"]
    Parse1 --> Norm{"normalize_original?"}

    Norm -->|Yes| NormStep["Normalize: transliterate script"]
    Norm -->|No| Call2["CALL 2: Critique"]
    NormStep --> Call2
    Reuse --> Call2

    Call2 --> Resolve2["resolve_prompt(evaluation_prompt)"]
    Resolve2 --> LLM2["llm.generate_with_audio(prompt, audio, schema)"]
    LLM2 --> Parse2["parse_critique_response()"]
    Parse2 --> Save["Save EvalRun to DB"]

    style Start fill:#6366f1,color:#fff
    style Call1 fill:#f59e0b,color:#fff
    style Call2 fill:#ec4899,color:#fff
    style Save fill:#10b981,color:#fff
        </div>
      </div>
    </div>
  </div>

  <!-- PIPELINE VIEW 4: Batch Pipeline -->
  <div id="pipeline-batch" style="display:none">
    <h3 class="section-title">Batch Evaluation Pipeline</h3>

    <div class="card">
      <div class="card-body">
        <div class="mermaid">
flowchart TD
    Start["run_batch_evaluation()"] --> LoadCSV["DataLoader: parse CSV"]
    LoadCSV --> Resolve["Resolve thread IDs (all / sample / selected)"]
    Resolve --> Loop["For each thread_id"]
    Loop --> GetThread["loader.get_thread(id)"]
    GetThread --> Intent["IntentEvaluator.evaluate_thread()"]
    Intent --> Correct["CorrectnessEvaluator.evaluate_thread()"]
    Correct --> Effic["EfficiencyEvaluator.evaluate_thread()"]
    Effic --> CustomEvals["Custom evaluators (resolve_prompt + generate_json)"]
    CustomEvals --> SaveThread["Save ThreadEvaluation row"]
    SaveThread -->|"Next thread"| Loop
    SaveThread -->|"All done"| Summary["Compute summary + save EvalRun"]

    style Start fill:#6366f1,color:#fff
    style Loop fill:#8b5cf6,color:#fff
    style Summary fill:#10b981,color:#fff
        </div>
      </div>
    </div>
  </div>

  <!-- PIPELINE VIEW 5: Adversarial Pipeline -->
  <div id="pipeline-adversarial" style="display:none">
    <h3 class="section-title">Adversarial Testing Pipeline</h3>

    <div class="card">
      <div class="card-body">
        <div class="mermaid">
flowchart TD
    Start["run_adversarial_evaluation()"] --> Gen["Phase 1: Generate Test Cases"]
    Gen -->|"AdversarialEvaluator.generate_test_cases(n)"| Cases["Test Cases (categories + difficulty)"]
    Cases --> Loop["For each test case"]
    Loop --> Conv["Phase 2: ConversationAgent.run_conversation()"]
    Conv -->|"Multi-turn SSE chat"| Kaira["Live Kaira API"]
    Kaira --> Transcript["Conversation Transcript"]
    Transcript --> Judge["Phase 3: evaluate_transcript()"]
    Judge -->|"LLM judges safety/compliance"| Verdict["Verdict + goal_achieved"]
    Verdict --> SaveAdv["Save AdversarialEvaluation row"]
    SaveAdv -->|"Next case"| Loop
    SaveAdv -->|"All done"| Summary["Summary: verdict distribution + goal counts"]

    style Start fill:#6366f1,color:#fff
    style Gen fill:#f59e0b,color:#fff
    style Conv fill:#ec4899,color:#fff
    style Judge fill:#8b5cf6,color:#fff
    style Summary fill:#10b981,color:#fff
        </div>
      </div>
    </div>
  </div>

</div>
